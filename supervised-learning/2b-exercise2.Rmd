---
title: "Bagging, Random Forests and Boosting: Exercise"
output: html_notebook
---

This notebook walks you through the exercise for the supervised learning II session.

## Setup

```{r}
library(caret)
library(randomForest)
library(xgboost)
```

In this exercise, we again use the reduced version of the BACH data set.

```{r}
load("BACH.Rda")
```

## Data preparation

Our machine learning task is -- again -- to predict whether a corporation experiences a net loss in the year 2015. Therefore, we first compute a dummy variable which indicates a loss with "1" and returns a "0" otherwise, based on the variable `bach$net_profit_or_loss`.

```{r}
bach$D_loss <- ifelse(bach$net_profit_or_loss < 0, 1, 0)
bach$D_loss <- as.factor(bach$D_loss)
summary(bach$D_loss)
```

Then we split the data set into a training and test part, using the year 2015 for the test set.

```{r}
bach_test <- bach[bach$year == "2015",]
bach_train <- bach[bach$year != "2015",]
```

## Random Forest

We may want to grow a random forest as a first classifier, using `caret`. This time, we want to be explicit about our data structure in the Cross-Validation process. For this, take a look at the function `groupKFold` and apply it as needed here.

```{r}
folds <- groupKFold(bach_train$year)
```

The resulting object from `groupKFold` can be passed to the `index` argument of the `trainControl` function. Furthermore, specify Cross-Validation as the evaluation method for model tuning.

```{r}
ctrl  <- trainControl(method = "cv",
                      number = 15,
                      index = folds)
```

Now we can use `train` from `caret` in order to grow the forest. Use the binary loss variable as the outcome and `~ . - net_profit_or_loss - return_on_equity` on the right hand side of the function call.

```{r}
rf <- train(D_loss ~ . - net_profit_or_loss - return_on_equity,
            data = bach_train,
            method = "rf",
            trControl = ctrl,
            importance = TRUE,
            metric = "Kappa")
```

Here we can add some code for inspecting the random forest results.

```{r}
rf
plot(rf)
plot(rf$finalModel)
varImp(rf)
```

### Boosting

We may want to use Boosting as an additional prediction method. When using `xgboost`, it is useful to specify a tuning grid first.

```{r}
grid <- expand.grid(max_depth = 1:3,
                    nrounds = c(500, 1000),
                    eta = c(0.05, 0.01),
                    min_child_weight = 5,
                    subsample = 0.7,
                    gamma = 0,
                    colsample_bytree = 1)

grid
```

Now we can pass this grid to `train`, using `xgbTree` as the machine learning method. Many arguments can be copied from the previous call to `train`.

```{r}
xgb <- train(D_loss ~ . - net_profit_or_loss - return_on_equity,
             data = bach_train,
             method = "xgbTree",
             trControl = ctrl,
             tuneGrid = grid,
             metric = "Kappa")
```

Again, take a look at the results from the tuning process, e.g. by printing and/or plotting the corresponding object.

```{r}
xgb
plot(xgb)
varImp(xgb)
```

## Prediction

Finally, we can use `predict` in order to predict class membership in the test set based on the results from both classifiers.

```{r}
rf_class <- predict(rf, newdata = bach_test)
xgb_class <- predict(xgb, newdata = bach_test)
```

Given predicted class membership, we can use `confusionMatrix` for evaluating prediction performance.

```{r}
confusionMatrix(rf_class, bach_test$D_loss)
confusionMatrix(xgb_class, bach_test$D_loss)
```
